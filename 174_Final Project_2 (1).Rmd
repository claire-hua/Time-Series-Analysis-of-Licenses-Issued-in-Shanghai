---
title: "174 Project"
author: "Claire Hua (9952425)"
date: "12/5/2018"
output: pdf_document
---

```{r,echo=F}
#install.packages("robustbase")
```

### Abstract

The purpose of this project is to work with data based off of the monthly Shanghai auction system to sell a limited number of license plates to fossil-fuel car buyers. The data has been constantly collected every month since January of 2002 and continues to be updated to this day. Throughout the project, we use various forms of time series techniques and methods to analyze the features of the data. These methods include ACF, PACF, log transformation, square root transformation, box-cox transformation, differencing, AIC for model comparison, and back transformation. We also use the information to help us forecast the predictions of the license plate proportions up until the year 2020. After making the time series forecast and analysis of the data set, we come to the conclusion that the monthly Shanghai proportion of license plates for fossil-fuel car buyers will continue to grow at a very slow rate.

### Introduction

For the data we are analyzing, we are concentrating on the prediction of monthly auction sales of license plates in Shanghai for fossil-fuel car buyers. Our data begins in January 2002 and is continuously updated each month. We forecast the monthly proportion of licenses issued and the number of applicants up until the year 2020 to determine whether the proportion of licenses issued to numer of applicants will increase or decrease as time goes on. The license plate in Shanghai is referred to as "the most expensive piece of metal in the world" and the average price is about $13,000. Due to Shanghai's increasing air pollution problem, this was the government's solution to attempt to combaat the problem.

Our data contains the following variables: 

Total Number of Licenses Issued = Number of licenses issued per year
Lowest Price = Price of the lowest auctioned license plate per year
Average Price = Average price of a license plate per year
Total Number of Applicants = Number of people applying for license plates issues per year
Date = Monthly dates starting at January, 2002 when the license plates are issued

We are planning on using time series techniques to predict the coming monthly proportion as well as back-transform to predict information that has already past. After all analysis is complete, we can see that our predictions tell us that the proportion of licenses issued to the number of applicants increases very slowly over time.

```{r setup,include=F,warning=F}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
#install.packages("robustbase")
library(robustbase)
#install.packages("qpcR")
library(qpcR)
library(rgl)
#install.packages("MuMIn")
library(MuMIn)
#install.packages("forecast")
library(forecast)
```
### Initial Analysis

We first convert the data into a time series and plot each of the four variables, lowest price, total number of licenses issued, average price, and total number of applicants. For the plot of lowest price, we can see that for about half the plot, the price seems to be slowly increasing with a little fluctuation. There then seems to be a sudden spike in which the lowest price increases significantly. For the plot of total number of licenses issued, we can see that the number issued is partially consistant with little increase as time goes on. There are however instances in which the number of licenses issued is dramatically changed, as we can see around 75, and the decrease from approximately 145 to 175. For the plot of average price, we can see that there is an upward trend and for the plot of total number of applicants, we can see that it is a low amount up until approximately 150. At this point in time, the number of applicants begins to increase dramatically and then becomes constant at around 250,000, but then seems to begin to drop back down again.

```{r initial analysis,echo=F}
shanghai <- read.csv("/Users/cyan/Documents/PSTAT 174/shanghai.csv", header = T)
#convert data into time series format
#gives proportion of licenses issued bc does total minus number of applicants
shanghai_prop = ts(shanghai[,2]/shanghai[,5], start=c(2002,01), frequency = 12)
op<-par(mfrow = c(2,2))
ts.plot(shanghai$X.lowest.price, main = "Lowest Price of Licenses Issued Per Year", ylab="Lowest Price of Licenses", xlab = "Time")
ts.plot(shanghai$Total.number.of.license.issued,main = "Total Number of Licenses Issued Per Year", ylab="Total Number of Licenses", xlab = "Time")
ts.plot(shanghai$avg.price,main = "Average Price of Licenses Issued Per Year", ylab="Average Price of Licenses", xlab = "Time")
ts.plot(shanghai$Total.number.of.applicants,main = "Total Number of People Applying for Licenses Issued Per Year", ylab="Total Number of People Applying for Licenses", xlab = "Time")
par(op)
```


```{r, echo=F}
mean(shanghai_prop)
var(shanghai_prop)
#time series plot
#variance doesnt seem constant bc it tails off
ts.plot(shanghai_prop, main = "Monthly Proportion of Licenses Issued in Shanghai (2002-2018)", xlab = "Time", ylab="Monthly Proportion of Licenses Issued")
op <- par(mfrow=c(1,2))
acf(shanghai_prop,main="", xlim=c(0,3))
pacf(shanghai_prop,ylab="PACF",main="", xlim=c(0,3))
title("ACF and PACF of Proportion of Shanghai-Issued License Plates",outer=T,line=-1)
par(op)
#the lags correspond to time period where lag=1 is also lag=12

```

We continue by finding the mean and variance of the proportion of licenses issued to total number of applicants. We get values of 0.375632 for the mean and 0.0609048 for the variance. Then, once we have plotted the time series of the proportion of monthly licenses issued, we see that it is not stationary. We then use ACF and PACF plots to attempt to hypothesize the type of series we are working with. The ACF seems to cut off before lag 2 while the PACF tails off starting near lag 0.7. So with this information, we can hypothesize that the original series is that of an AR model.

Note: In our ACF and PACF plots, our Lags are in increments of years such that Lag 1 = 12 months and Lag 2 = 24 months.

### Transformations

We first begin the transformation by testing to see which of the three forms of transformations works best in our situation. We are choosing between Box-cox tranformation, Log transformation, and Square root transformation. We then plot each of the tranformations and compare them to our original plot.

We applied the transformation because our initial time series was not stationary. Due to heteroscedasticity, our original time series violated our constant error of variance assumption. This is because our variance of error appeared to be changing over time.

```{r,echo=F}
# three transformations (boxcox, log. sqrt)
library(MASS)
t = 1:length(shanghai_prop)
fit = lm(shanghai_prop ~ t)
bcTransform = boxcox(shanghai_prop ~ t,plotit = TRUE)
#choose a lambda of 1/2
op <- par(mfrow = c(2,2)) 
#max point on the bc graph (should be .46)
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
lambda
shanghai_prop.bc = (1/lambda)*(shanghai_prop^lambda-1) #boxcox
shanghai_prop.log <- log(shanghai_prop)#log 
shanghai_prop.sqrt <- sqrt(shanghai_prop) # sqrt

#comparing original and transformed data
ts.plot(shanghai_prop, main = "Original Data")
ts.plot(shanghai_prop.bc,main = "Box-Cox") 
ts.plot(shanghai_prop.log,main = "Log")
ts.plot(shanghai_prop.sqrt,main = "Square-Root Transformed Data") 
par(op)
```

When looking at all the plots above, we quickly realize that the graphs are difficult to interpret and so we find the variances of each to determine which is the best fit for our model. Based off of the results, we can see that the square root transformation gives us the smallest variance value and therefore we determine that this is the best transformation for our model. Also, the box-cox transformation tells us that lambda is `r 0.46` which is relatively close to 0.5, which tells us that the square-root transformation performs best.

```{r,echo=F}
#bc graphs are very difficult to understand, we find variance to determine which to use
print(paste("Variance of Original Time Series:",var(shanghai_prop)))
print(paste("Variance of Box-Cox Transformation:",var(shanghai_prop.bc)))
print(paste("Variance of Log Transformation:",var(shanghai_prop.log)))
print(paste("Variance of Square Root Transformation", var(shanghai_prop.sqrt)))
 #sqrt transformation has the smallest variance so we choose this
```

### Square Root Transformation

Continuing with our chosen Square Root Transformaton, we plot the ACF and PACF time series and we see that the ACF is still tailing off while the PACF cuts of at around lag 0.8, which further supports our initial assumption that the series follows an AR(p) model.

```{r,echo=F}

op <- par(mfrow=c(1,2))
acf(shanghai_prop.sqrt, main="",xlim=c(0,3))
pacf(shanghai_prop.sqrt, ylab="PACF", main="",xlim=c(0,3))
title("ACF and PACF Square-Root Transformed Time Series ",outer=T,line=-1)
par(op)

```

###Differencing to Remove Seasonality

After applying the square root transformation, our data still does not look stationary. Therefore, we will apply differencing to remove trends and seasonality. 
We difference once at lag 12 to remove the seasonality component so that the de-seasonalized data fluctuates around the mean=0 line. For the ACF, we can see that it begins to slowly decay while the PACF oscillates between the bounds.

```{r,echo=F}
#difference at lag=12 to remove seasonality component
#want to be stationary to want it to fluctuate around the blue line whihc is mean = 0
shanghai_prop.diff12 <- diff(shanghai_prop.sqrt,12)
var(shanghai_prop.diff12)
ts.plot(shanghai_prop.diff12, main = "De-seasonalized data for Shanghai",ylab=expression(nabla~Y[t]))
abline(h = 0,lty = 2,col="blue")

op <- par(mfrow=c(1,2))
acf(shanghai_prop.diff12,main="")
pacf(shanghai_prop.diff12,ylab="PACF",main="")
title("Shanghai proportion of license plates, differenced at lag 12",outer=T,line=-1)
par(op)
```

```{r,echo=F}
#difference at lag 1 to remove trend
shanghai_prop.diff1 <- diff(shanghai_prop.sqrt, 1)
var(shanghai_prop.diff1)
ts.plot(shanghai_prop.diff1, main = "De-trended and De-seasonalized data forproportion of license plates",ylab=expression(nabla~Y[t]))
abline(h = 0,lty = 2,col="blue")

op <- par(mfrow=c(1,2))
acf(shanghai_prop.diff1,main="")
pacf(shanghai_prop.diff1,ylab="PACF",main="")
title("ACF and PACF of proportion of license plates, differenced at lag 1",outer=T,line=-1)
par(op)

#differencing at lag 1 twice increased the variance therefore we choose to difference once
shanghai_prop.diff2 <- diff(shanghai_prop.sqrt, 1, 2)
var(shanghai_prop.diff2)
ts.plot(shanghai_prop.diff2, main = "Proportion of license plates after twice differenced at lag 1",ylab=expression(nabla~Y[t]))
abline(h = 0,lty = 2,col="blue")
```
We difference again at lag 1 to remove the trend component of the data. This gives us a de-trended and de-seasonalized series to work with. The first time we difference at lag 1, we get a variance value of `0.01220669` but when we difference a second time at lag 1, our variances increases to `0.03093042` and so this tells us to only difference at lag 1 once. We can see that our de-trended and de-seasonalized data plot is now fluctuating very closely around the mean = 0 line which shows that it is stationary. Our ACF plot oscillates between the bounds while the PACF seems to cut off at lag 0.1.

### Parameter Estimation using Yule-Walker

We perform preliminary estimation using Yule-Walker and it gives us an AR model of order 10, so this may be an AR(10) process.

```{r,echo=F}
# Preliminary estimation using Yule-Walker
ar(shanghai_prop.diff1, method="yule-walker") #AR(10)
```

### Fitting an ARMA Process

Using the auto.arima() function, we find that the estimated model is a ARIMA (1,0,1) model and so we use the estimated orders of (p,q) to run further AIC tests and find the best model. 

```{r,echo=F}
library(forecast)
#auto.arima is used to give us the best model
fit_arma <-auto.arima(shanghai_prop.diff1, stationary = TRUE, seasonal = FALSE)
fit_arma
```
### ARMA Models

Using a for-loop, we test each of the possible ARMA(p,q) parameter values to see which process gives us the smallest value of AIC. Looking at our results, we can see that ARMA(1,1) gives us the lowest AIC value of -217.3153.

```{r,echo=F} 
#gives us the model with the smallest AIC (should be ARMA (1,1))
#running for loops to test all parameter values of ARMA(p,q)
for (i in 0:1) {
  for (j in 0:1) {
    print(i)
    print(j)
    print(AICc(arima(shanghai_prop, order = c(i,0,j), method = "ML")))
  }
}
```
### Checking for the best model fit

We do further testing to see if our model can be reduced more. 
We begin to check each of the three possible models: AR(1), MA(1), and ARMA(1,1) to see which is the best fit. We fit each of the three and then test each individual AIC to see which produces the lowest value. Our results shows us that ARMA(1,1) has returned the lowest AIC of -346.0987 while MA(1) gives us -106.3236 and AR(1) gives us -201.9093. Therefore, we can conclude that an ARMA(1,1) model is best for our data.

```{r,echo=F}
#comparing ARMA(1,1), AR(1), MA(1) to find best model
fit_ar1 <- arima(shanghai_prop, order=c(1,0,0), method="ML")
fit_ma1 <- arima(shanghai_prop, order=c(0,0,1), method="ML")
fit_arma11 <- arima(shanghai_prop.diff1, order=c(1,0,1), method="ML")
AICc(fit_ar1)
AICc(fit_ma1)
AICc(fit_arma11) #minimum AIC value of -335.8137
```
### Plotting Residuals of ARMA(1,1)

After deciding that ARMA(1,1) is the best model, we then plot the residuals. We can see that the residuals seem to oscillate about the line at error 0.

```{r, echo=F}
#plotting residuals of ARMA(1,1)
err <- residuals(fit_arma11)
plot(err, main="Residuals of ARMA(1,1) Process")
abline(h=0,lty=2,col="blue")
```
### Diagnostic Testing for Normality of Residuals

We perform diagnostic checking to check for the normality of errors, if the residuals are serially correlated, and if the residuals are not heteroskedastic and have constant variance. 

The Shapiro-Wilk test gives us a p-value of 3.169e-12 which is less than our alpha of 0.05, so we conclude that the ARMA(1,1) does not pass the Shapiro Wilk test.

The Ljung-Box test for constant variance gives us a p-value of 0.7762 which is greater than our alpha of 0.05, so we can accept the assumption of normality and conclude that the residuals are random.

The Box-Pierce test gives us a p-value of p-value = 0.7778, which is very similar to that of the Ljung-Box test, and since that value is greater than out alpha of 0.05, we can conclude that the residuals are serially correlated.

We also plot a QQ-Plot and from that we can see that the errors follow the diagonal line, and so we can assume that the errors are normally distributed. Our histogram shows that our data is normally distributed.

```{r,echo=F}
#Diagnostic Checking for normality of residuals
#Shapiro Wilk Test
shapiro.test(err) #significant p-value 
#Ljung-Box Test - tests for constant variance of residuals
Box.test(err, type = "Ljung") #do not reject the assumption of normal so the residuals are not highly correlated and are therefore random
#Box Pierce
Box.test(err, type = "Box-Pierce") #The residuals are serially correlated as p>.05
#histogram
hist(err)
#qq plot
qqnorm(err)
qqline(err, col = "blue")
```

### Forecasting

Since we have completed identifying the proper model, estimated the parameters, and gone through diagnostic checks, we can now move on towards forecasting the data. We are going to use forecasting to predict the proportional value of licenses issued to number of applicants for the next two years. Since we transformed our data using a Square root transformation, we will need to find the predicted values and then back-transform to forecast our raw data. We used our ARMA(1,1) model and forecasted the next 24 months. We also calculated an upper and lower confidence interval to calculate a 95% confidence interval for the predicted values. 

```{r,echo=F}
library(forecast)
#forecasts 2 years ahead
pred.tr <- predict(fit_arma11, n.ahead = 24, newreg = length(shanghai_prop.diff1)+1:length(shanghai_prop.diff1)+24)
pred.tr
```

```{r,echo=F}
U.tr = pred.tr$pred + 1.96*pred.tr$se
L.tr = pred.tr$pred - 1.96*pred.tr$se
ts.plot(shanghai_prop.diff1,ylab="Proportion issued, differenced")
abline(h=0,lty=2,col="blue")
```

```{r,echo=F}
U.tr
L.tr
op <- par(mfrow=c(1,1))
ts.plot(shanghai_prop, xlim = c(2002,2020), ylim = c(-.5,1), type = 'l', main = "Forecast of Proportion of Shanghai Issued License Plates",ylab="Proportion Issued")
points(pred.tr$pred, col = "red")
max(U.tr)
lines(U.tr, col = "blue",lty = "dashed")
lines(L.tr, col = "red",lty = "dashed")
```
### Back-Transform

ASK PROF FIRST

```{r,echo=F}
#####ask Bapat
#Back Transformation
#x = exp(log(alpha * transform + 1) / alpha)
#pred.orig <- ((pred.tr$pred)*lambda + 1)^(1/lambda)
shanghai_prop.sq <- shanghai_prop.sqrt^2
pred.sq <- predict(fit_arma11, n.ahead = 24, newreg = length(shanghai_prop.sq)+1:length(shanghai_prop.sq)+24)
pred.sq
pred.orig <- ((pred.sq$pred)*lambda + 1)^(1/lambda)
pred.orig
U.sq = pred.sq$pred + 1.96*pred.sq$se
L.sq = pred.sq$pred - 1.96*pred.sq$se
U = ((U.sq)*lambda + 1)^(1/lambda)
L = ((L.sq)*lambda + 1)^(1/lambda)
#time series from 2010-2020
ts.plot(shanghai_prop, xlim = c(2010, 2020), ylim = c(-.5,1), type = "l",ylab="Proportion issued")
points(pred.orig, col="red")
lines(U, col = "blue", lty = "dashed")
lines(L, col = "blue", lty = "dashed")
#time series for 2002-2020
ts.plot(shanghai_prop, xlim = c(2002, 2020), ylim = c(-.5,1), type = "l",ylab="Proportion issued")
points(pred.orig, col="red")
lines(U, col = "blue", lty = "dashed")
lines(L, col = "blue", lty = "dashed")
```

### Conclusion

To conclude, we used monthly data to analyze the proportion of Shanghai license plates issued per month to total number of applicants from 2002 to 2018. After transforming and differencing our dataset so that our data is stationary, we used the Yule-Walker method, a for-loop to compare AIC values, and the `auto.arima()` function to conclude that ARMA(1,1) was the best model. After determining our best model, we forecasted values for the next 2 years and found that the predicted values which are closer to 0 show that the proportion will stay relatively consistent as time goes on. There is a consistent trend as numbers of licenses issued and number of applicants continue to fluctuate as months go on. In other words, if the number of licenses plates issued increases, the number of applicants will adjust accordingly for the proportion to be stable. This is also true if the number of license plates decreases. 

Our results directly relate to the environmental problem at hand where our goal is to contain or reduce pollution. Thus, if a certain number of people apply for a license plate per month, then the Shanghai government attempts to regulate the number of license plates by proportionally reducing the number of license plates available at auction.
